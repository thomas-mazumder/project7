{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd137d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "3faaef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a neural network class that generates a fully connected Neural Network.\n",
    "\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            This list of dictionaries describes the fully connected layers of the artificial neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}] will generate a\n",
    "            2 layer deep fully connected network with an input dimension of 64, a 32 dimension hidden layer\n",
    "            and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning Rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            This list of dictionaries describing the fully connected layers of the artificial neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 nn_arch: List[Dict[str, Union[int, str]]],\n",
    "                 lr: float,\n",
    "                 seed: int,\n",
    "                 batch_size: int,\n",
    "                 epochs: int,\n",
    "                 loss_function: str):\n",
    "        # Saving architecture\n",
    "        self.arch = nn_arch\n",
    "        # Saving hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "        # Initializing the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD!! IT IS ALREADY COMPLETE!!\n",
    "\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "        # seeding numpy random\n",
    "        np.random.seed(self._seed)\n",
    "        # defining parameter dictionary\n",
    "        param_dict = {}\n",
    "        # initializing all layers in the NN\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            # initializing weight matrices\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            # initializing bias matrices\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "        return param_dict\n",
    "\n",
    "    def _single_forward(self,\n",
    "                        W_curr: ArrayLike,\n",
    "                        b_curr: ArrayLike,\n",
    "                        A_prev: ArrayLike,\n",
    "                        activation: str) -> Tuple[ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single forward pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            activation: str\n",
    "                Name of activation function for current layer.\n",
    "\n",
    "        Returns:\n",
    "            A_curr: ArrayLike\n",
    "                Current layer activation matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transformed matrix.\n",
    "        \"\"\"\n",
    "        Z_curr = np.transpose(np.matmul(W_curr, np.transpose(A_prev)))\n",
    "        #print(f'_single_forward: b_curr is {b_curr.shape}')\n",
    "        Z_curr = Z_curr + b_curr.flatten()\n",
    "        if activation == \"sigmoid\":\n",
    "            A_curr = self._sigmoid(Z_curr) \n",
    "        elif activation == \"relu\":\n",
    "            A_curr = self._relu(Z_curr)\n",
    "        return A_curr, Z_curr\n",
    "\n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        \"\"\"\n",
    "        This method is responsible for one forward pass of the entire neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input matrix with shape [batch_size, features].\n",
    "\n",
    "        Returns:\n",
    "            output: ArrayLike\n",
    "                Output of forward pass.\n",
    "            cache: Dict[str, ArrayLike]:\n",
    "                Dictionary storing Z and A matrices from `_single_forward` for use in backprop.\n",
    "        \"\"\"\n",
    "        A_prev = X\n",
    "        cache = {'0': (A_prev,)}\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            W_curr = self._param_dict['W' + str(idx + 1)]\n",
    "            b_curr = self._param_dict['b' + str(idx + 1)]\n",
    "            #print(f'forward: layer is {layer}')\n",
    "            #print(f'forward: b_curr is {b_curr.shape}')\n",
    "            activation = layer[\"activation\"]\n",
    "            A_prev = cache[str(idx)][0]\n",
    "            cache[str(idx + 1)] = self._single_forward(W_curr, b_curr, A_prev, activation)\n",
    "        n_layers = len(self.arch)\n",
    "        output = cache[str(n_layers)][0]\n",
    "        return output, cache\n",
    "\n",
    "    def _single_backprop(self,\n",
    "                         W_curr: ArrayLike,\n",
    "                         b_curr: ArrayLike,\n",
    "                         Z_curr: ArrayLike,\n",
    "                         A_prev: ArrayLike,\n",
    "                         dA_curr: ArrayLike,\n",
    "                         activation_curr: str) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This method is used for a single backprop pass on a single layer.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Current layer weight matrix.\n",
    "            b_curr: ArrayLike\n",
    "                Current layer bias matrix.\n",
    "            Z_curr: ArrayLike\n",
    "                Current layer linear transform matrix.\n",
    "            A_prev: ArrayLike\n",
    "                Previous layer activation matrix.\n",
    "            dA_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer activation matrix.\n",
    "            activation_curr: str\n",
    "                Name of activation function of layer.\n",
    "\n",
    "        Returns:\n",
    "            dA_prev: ArrayLike\n",
    "                Partial derivative of loss function with respect to previous layer activation matrix.\n",
    "            dW_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer weight matrix.\n",
    "            db_curr: ArrayLike\n",
    "                Partial derivative of loss function with respect to current layer bias matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate the partial derivative of the current activation matrix\n",
    "        # with respect to the current linear output matrix (\"Z\")\n",
    "        \n",
    "        if activation_curr == \"relu\":\n",
    "            dJ_dZcurr = self._relu_backprop(dA_curr, Z_curr)\n",
    "        elif activation_curr == \"sigmoid\":\n",
    "            dJ_dZcurr = self._sigmoid_backprop(dA_curr, Z_curr)\n",
    "        \n",
    "        # calculate the partial derivate of the loss function with\n",
    "        # respect to the previous activation layer, dJ_dAprev \n",
    "        dZcurr_dAprev = W_curr\n",
    "        dJ_dAprev = np.array([np.matmul(dJ_dZcurr[i,:], dZcurr_dAprev) for i in range(dJ_dZcurr.shape[0])])\n",
    "        \n",
    "        # calculate the partial derivate of the loss function with\n",
    "        # respect to the bias of the current layer, dJ_dbcurr \n",
    "        dJ_dbcurr = np.sum(dJ_dZcurr, axis = 0) # (64,)\n",
    "        #print(f'_single_backprop: dJ_dbcurr is {dJ_dbcurr.shape}')\n",
    "        \n",
    "        # calculate the partial derivative of the loss function with\n",
    "        # respect to the weights of the current layer, dJ_dWcurr \n",
    "        dZcurr_dWcurr = np.tile(A_prev , (W_curr.shape[0], 1, 1)) # (64, 10, 16)\n",
    "        dJ_dZcurr = np.tile(dJ_dZcurr, (W_curr.shape[1], 1, 1)) # (16, 10, 64)\n",
    "        dJ_dZcurr = np.moveaxis(dJ_dZcurr, [2,0],[0,2]) # (64, 10, 16)\n",
    "        dJ_dWcurr = np.sum(dJ_dZcurr * dZcurr_dWcurr, axis = 1) # (64, 16)\n",
    "        \n",
    "        return dJ_dWcurr, dJ_dbcurr, dJ_dAprev\n",
    "        \n",
    "                \n",
    "    def backprop(self, y: ArrayLike, y_hat: ArrayLike, cache: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This method is responsible for the backprop of the whole fully connected neural network.\n",
    "\n",
    "        Args:\n",
    "            y (array-like):\n",
    "                Ground truth labels.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output values.\n",
    "            cache: Dict[str, ArrayLike]\n",
    "                Dictionary containing the information about the\n",
    "                most recent forward pass, specifically A and Z matrices.\n",
    "\n",
    "        Returns:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from this pass of backprop.\n",
    "        \"\"\"\n",
    "        n_layers = len(layers)\n",
    "        grad_dict = dict()\n",
    "        \n",
    "        if self._loss_func == \"BCE\":\n",
    "            dJ_dAcurr = self._binary_cross_entropy_backprop(y, y_hat)\n",
    "        elif self._loss_func == \"MSE\":\n",
    "            dJ_dAcurr = self._mean_squared_error_backprop(y, y_hat)\n",
    "        \n",
    "        for idx, layer in enumerate(reversed(layers)):\n",
    "            W_curr = self._param_dict[\"W\" + str(n_layers - idx)]\n",
    "            b_curr = self._param_dict[\"b\" + str(n_layers - idx)]\n",
    "            Z_curr = cache[str(n_layers - idx)][1]\n",
    "            A_prev = cache[str(n_layers - idx - 1)][0]    \n",
    "            dA_curr = dJ_dAcurr\n",
    "            activation_curr = layer[\"activation\"]\n",
    "            dJ_dWcurr, dJ_dbcurr, dJ_dAprev = self._single_backprop(W_curr, b_curr, Z_curr, \n",
    "                                                               A_prev, dA_curr, activation_curr)\n",
    "            grad_dict[\"W\" + str(n_layers - idx)] = dJ_dWcurr\n",
    "            grad_dict[\"b\" + str(n_layers - idx)] = dJ_dbcurr\n",
    "            dJ_dAcurr = dJ_dAprev\n",
    "            \n",
    "        return grad_dict\n",
    "\n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This function updates the parameters in the neural network after backprop. This function\n",
    "        only modifies internal attributes and thus does not return anything\n",
    "\n",
    "        Args:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from most recent round of backprop.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for idx in range(1, len(self.arch) + 1):\n",
    "            self._param_dict[\"W\" + str(idx)] = self._param_dict[\"W\" + str(idx)] - self._lr * grad_dict[\"W\" + str(idx)]\n",
    "            self._param_dict[\"b\" + str(idx)] = self._param_dict[\"b\" + str(idx)].flatten() - self._lr * grad_dict[\"b\" + str(idx)]\n",
    "            \n",
    "    def fit(self,\n",
    "            X_train: ArrayLike,\n",
    "            y_train: ArrayLike,\n",
    "            X_val: ArrayLike,\n",
    "            y_val: ArrayLike) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        This function trains the neural network via training for the number of epochs defined at\n",
    "        the initialization of this class instance.\n",
    "        Args:\n",
    "            X_train: ArrayLike\n",
    "                Input features of training set.\n",
    "            y_train: ArrayLike\n",
    "                Labels for training set.\n",
    "            X_val: ArrayLike\n",
    "                Input features of validation set.\n",
    "            y_val: ArrayLike\n",
    "                Labels for validation set.\n",
    "\n",
    "        Returns:\n",
    "            per_epoch_loss_train: List[float]\n",
    "                List of per epoch loss for training set.\n",
    "            per_epoch_loss_val: List[float]\n",
    "                List of per epoch loss for validation set.\n",
    "        \"\"\"\n",
    "        per_epoch_loss_train = []\n",
    "        per_epoch_loss_val = []\n",
    "        for t in range(self._epochs):\n",
    "            this_epoch_loss_train = []\n",
    "            n_batches = np.ceil(len(y_train)/self._batch_size)\n",
    "            shuffler = np.random.permutation(X_train.shape[0])\n",
    "            X_train = X_train[shuffler,:]\n",
    "            y_train = y_train[shuffler]\n",
    "            count = 0\n",
    "            # train the network in batches\n",
    "            for X_train_batch, y_train_batch in zip(np.array_split(X_train, n_batches), np.array_split(y_train, n_batches)):\n",
    "                output, cache = self.forward(X_train_batch)\n",
    "                if self._loss_func == \"BCE\":\n",
    "                    this_epoch_loss_train.append(self._binary_cross_entropy(y_train_batch, output))\n",
    "                elif self._loss_func == \"MSE\":\n",
    "                    this_epoch_loss_train.append(self._mean_squared_error(y_train_batch, output))\n",
    "                grad_dict = self.backprop(y_train_batch, output, cache)\n",
    "                self._update_params(grad_dict)\n",
    "                count += 1\n",
    "            per_epoch_loss_train.append(np.mean(this_epoch_loss_train))\n",
    "            \n",
    "            # compute validation loss\n",
    "            output, _ = self.forward(X_val)\n",
    "            if self._loss_func == \"BCE\":\n",
    "                per_epoch_loss_val.append(self._binary_cross_entropy(y_val, output))\n",
    "            elif self._loss_func == \"MSE\":\n",
    "                per_epoch_loss_val.append(self._mean_squared_error(y_val, output))\n",
    "            \n",
    "                \n",
    "            \n",
    "        return per_epoch_loss_train, per_epoch_loss_val\n",
    "                \n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        This function returns the prediction of the neural network model.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            y_hat: ArrayLike\n",
    "                Prediction from the model.\n",
    "        \"\"\"\n",
    "        output, _ = self.forward(X)\n",
    "        return output\n",
    "\n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        Z[Z < 0] = 0\n",
    "        return Z\n",
    "\n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike):\n",
    "        \"\"\"\n",
    "        Sigmoid derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dJ_dZcurr: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        dAcurr_dZcurr = self._sigmoid(Z) * (1 - self._sigmoid(Z))\n",
    "        dJ_dZcurr = dA * dAcurr_dZcurr\n",
    "        \n",
    "        return dJ_dZcurr\n",
    "\n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        ReLU derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dJ_dZcurr: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        dAcurr_dZcurr = Z\n",
    "        dAcurr_dZcurr[Z >= 0] = 1\n",
    "        dAcurr_dZcurr[Z < 0] = 0\n",
    "        dJ_dZcurr = dA * dAcurr_dZcurr\n",
    "        \n",
    "        return dJ_dZcurr\n",
    "        \n",
    "\n",
    "    def _binary_cross_entropy(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss over mini-batch.\n",
    "        \"\"\"\n",
    "        y_hat[y_hat == 0] = 0.0001\n",
    "        y_hat[y_hat == 1] = 0.9999\n",
    "        term1 = y * np.log(y_hat)\n",
    "        term2 = (1 - y) * np.log(1 - y_hat)\n",
    "        return -np.mean(term1 + term2)\n",
    "\n",
    "    def _binary_cross_entropy_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function derivative.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        y_hat[y_hat == 0] = 0.0001\n",
    "        y_hat[y_hat == 1] = 0.9999\n",
    "        num = y_hat - y\n",
    "        den = y_hat * (1 - y_hat)\n",
    "        dJ_dAcurr = num/den\n",
    "        \n",
    "        return dJ_dAcurr\n",
    "\n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Mean squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss of mini-batch.\n",
    "        \"\"\"\n",
    "        return np.mean(np.square(y - y_hat))\n",
    "\n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Mean square error loss derivative.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        return -2 * (y - y_hat) / y.shape[0]\n",
    "\n",
    "    def _loss_function(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Loss function, computes loss given y_hat and y. This function is\n",
    "        here for the case where someone would want to write more loss\n",
    "        functions than just binary cross entropy.\n",
    "\n",
    "        Args:\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss of mini-batch.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _loss_function_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        This function performs the derivative of the loss function with respect\n",
    "        to the loss itself.\n",
    "        Args:\n",
    "            y (array-like): Ground truth output.\n",
    "            y_hat (array-like): Predicted output.\n",
    "        Returns:\n",
    "            dA (array-like): partial derivative of loss with respect\n",
    "                to A matrix.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "20c71faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [{'input_dim': 64, 'output_dim': 16, 'activation': 'sigmoid'}, \n",
    "          {'input_dim': 16, 'output_dim': 64, 'activation': 'relu'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "f83b4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(layers, lr = 0.05, seed = 0, batch_size = 100, epochs = 10, loss_function = \"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "6b933e4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss, val_loss = nn.fit(X_train, X_train, X_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "204cc238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbd90290a30>]"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(10), train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "787e063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = nn.forward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "9d2a8cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbdd02cc190>"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMM0lEQVR4nO3d3Ytd1R3G8efpJJrUSAeTKNGJL4USEEETVCqKtBFFq9gLe5GASqXFCq0oLYj2pvgPBHtRhOBLBTWi0UCR1ipEEaHV5s0akzHEEDGJGo3RmGAaZvz14uyUmCbOnnGvNSfz+37gkDMz55xnTWaes/Y+s89ejggBmNq+M9kDAFAeRQcSoOhAAhQdSICiAwlQdCCBvii67Wttv2N7q+17C2c9Ynu37Y0lc47Im2/7ZdubbL9t+67CeTNsv2H7zSbv/pJ5TeaA7fW2ny+d1eRtt/2W7Q221xTOGrS90vaw7c22LyuYtaD5ng5f9tm+u5MHj4hJvUgakPSupO9LOknSm5LOL5h3paRFkjZW+v7mSVrUXD9V0pbC358lzWquT5f0uqQfFv4efyvpSUnPV/o/3S5pTqWsxyT9srl+kqTBSrkDkj6UdE4Xj9cPM/qlkrZGxLaIOCTpKUk/LRUWEa9K+rTU4x8j74OIWNdc/0LSZklnFcyLiNjffDi9uRQ7Ksr2kKTrJT1UKmOy2P6eehPDw5IUEYci4rNK8VdJejci3uviwfqh6GdJev+Ij3eoYBEmk+1zJS1Ub5YtmTNge4Ok3ZJeioiSeQ9IukfSVwUzjhaSXrS91vbtBXPOk/SxpEebXZOHbJ9SMO9ISySt6OrB+qHoKdieJelZSXdHxL6SWRExGhEXSRqSdKntC0rk2L5B0u6IWFvi8b/BFRGxSNJ1kn5t+8pCOdPU2817MCIWSjogqehrSJJk+yRJN0p6pqvH7Iei75Q0/4iPh5rPTRm2p6tX8ici4rlauc1m5suSri0UcbmkG21vV2+Xa7Htxwtl/U9E7Gz+3S1plXq7fyXskLTjiC2ileoVv7TrJK2LiI+6esB+KPq/JP3A9nnNM9kSSX+Z5DF1xrbV28fbHBHLKuTNtT3YXJ8p6WpJwyWyIuK+iBiKiHPV+7mtjoibS2QdZvsU26cevi7pGklF/oISER9Ket/2guZTV0naVCLrKEvV4Wa71Ns0mVQRMWL7N5L+rt4rjY9ExNul8myvkPQjSXNs75D0h4h4uFSeerPeLZLeavabJen3EfHXQnnzJD1me0C9J/KnI6LKn70qOUPSqt7zp6ZJejIiXiiYd6ekJ5pJaJuk2wpmHX7yulrSrzp93OalfABTWD9sugMojKIDCVB0IAGKDiRA0YEE+qrohQ9nnLQs8sib7Ly+Krqkmv+ZVX9w5JE3mXn9VnQABRQ5YMZ21aNwmqOkxiUiJnQ/SZozZ8647/Pll19q5syZE8o788wzx32fPXv2aPbs2RPKm8jvxLfJ27Vr17jvc/DgQc2YMWNCeXv37h33fb7N78vo6OiE7jdREfF/A530Q2C7MH369Kp5N910U9W8++8vfpKYrxkZGamaV/v7W7lyZdW8zz//vFrW8Z5U2HQHEqDoQAIUHUiAogMJUHQgAYoOJEDRgQQoOpBAq6LXXDIJQPfGLHpzksE/qXcK2vMlLbV9fumBAehOmxm96pJJALrXpuhplkwCpqrO3tTSvFG+9nt2AbTQpuitlkyKiOWSlkv136YK4Ju12XSf0ksmARmMOaPXXjIJQPda7aM364SVWisMQGEcGQckQNGBBCg6kABFBxKg6EACFB1IgKIDCVB0IIEpsVLLvHnzqubdcccdVfNOP/30qnnr16+vmnfJJZdUzVu9enXVvJortRwPMzqQAEUHEqDoQAIUHUiAogMJUHQgAYoOJEDRgQQoOpAARQcSaLMk0yO2d9veWGNAALrXZkb/s6RrC48DQEFjFj0iXpX0aYWxACiEfXQgAdZeAxLorOisvQb0LzbdgQTa/HlthaR/SFpge4ftX5QfFoAutVlkcWmNgQAoh013IAGKDiRA0YEEKDqQAEUHEqDoQAIUHUiAogMJTIm11+bOnVs1b2BgoGresmXLquYdOHCgat7g4GDVvNo/v9HR0ap5x8KMDiRA0YEEKDqQAEUHEqDoQAIUHUiAogMJUHQgAYoOJEDRgQTanBxyvu2XbW+y/bbtu2oMDEB32hzrPiLpdxGxzvapktbafikiNhUeG4COtFl77YOIWNdc/0LSZklnlR4YgO6Max/d9rmSFkp6vchoABTR+m2qtmdJelbS3RGx7xhfZ+01oE+1Krrt6eqV/ImIeO5Yt2HtNaB/tXnV3ZIelrQ5IuqeAQFAJ9rso18u6RZJi21vaC4/KTwuAB1qs/baa5JcYSwACuHIOCABig4kQNGBBCg6kABFBxKg6EACFB1IgKIDCUyJtddGRkaq5u3du7dq3vz586vmzZw5s2reli1bqub1w1potTGjAwlQdCABig4kQNGBBCg6kABFBxKg6EACFB1IgKIDCVB0IIE2Z4GdYfsN2282a6/dX2NgALrT5lj3/0haHBH7m/O7v2b7bxHxz8JjA9CRNmeBDUn7mw+nNxcWaABOIK320W0P2N4gabeklyKCtdeAE0irokfEaERcJGlI0qW2Lzj6NrZvt73G9pqOxwjgWxrXq+4R8ZmklyVde4yvLY+IiyPi4o7GBqAjbV51n2t7sLk+U9LVkoYLjwtAh9q86j5P0mO2B9R7Yng6Ip4vOywAXWrzqvu/JS2sMBYAhXBkHJAARQcSoOhAAhQdSICiAwlQdCABig4kQNGBBKbE2mv79+8f+0YdGh6uewTw0NBQ1bzTTjutat6uXbuq5k2bNiV+7ceFGR1IgKIDCVB0IAGKDiRA0YEEKDqQAEUHEqDoQAIUHUiAogMJtC56s4jDetucGBI4wYxnRr9L0uZSAwFQTtslmYYkXS/pobLDAVBC2xn9AUn3SPqq3FAAlNJmpZYbJO2OiLVj3I6114A+1WZGv1zSjba3S3pK0mLbjx99I9ZeA/rXmEWPiPsiYigizpW0RNLqiLi5+MgAdIa/owMJjOucOhHxiqRXiowEQDHM6EACFB1IgKIDCVB0IAGKDiRA0YEEKDqQAEUHEpgSi1Dt27evat7IyEjVvE2bNlXNO/vss6vmnXzyyVXzZs+eXTXPdrWsiDjm55nRgQQoOpAARQcSoOhAAhQdSICiAwlQdCABig4kQNGBBCg6kECrQ2CbUz1/IWlU0gindAZOLOM51v3HEfFJsZEAKIZNdyCBtkUPSS/aXmv79pIDAtC9tpvuV0TETtunS3rJ9nBEvHrkDZonAJ4EgD7UakaPiJ3Nv7slrZJ06TFuw9prQJ9qs5rqKbZPPXxd0jWSNpYeGIDutNl0P0PSquYsGdMkPRkRLxQdFYBOjVn0iNgm6cIKYwFQCH9eAxKg6EACFB1IgKIDCVB0IAGKDiRA0YEEKDqQwJRYe+3gwYNV80ZHR6vmXXhh3eOVZsyYUTVveHi4at6sWbOq5g0MDFTLOt66gMzoQAIUHUiAogMJUHQgAYoOJEDRgQQoOpAARQcSoOhAAhQdSKBV0W0P2l5pe9j2ZtuXlR4YgO60Pdb9j5JeiIif2T5J0ncLjglAx8Ysuu3vSbpS0s8lKSIOSTpUdlgAutRm0/08SR9LetT2etsPNQs5fI3t222vsb2m81EC+FbaFH2apEWSHoyIhZIOSLr36BuxJBPQv9oUfYekHRHxevPxSvWKD+AEMWbRI+JDSe/bXtB86ipJm4qOCkCn2r7qfqekJ5pX3LdJuq3ckAB0rVXRI2KDJPa9gRMUR8YBCVB0IAGKDiRA0YEEKDqQAEUHEqDoQAIUHUiAtdcmYOvWrVXzbr311qp5tqvmrV27tmrenj17qub1A2Z0IAGKDiRA0YEEKDqQAEUHEqDoQAIUHUiAogMJUHQggTGLbnuB7Q1HXPbZvrvC2AB0ZMxDYCPiHUkXSZLtAUk7Ja0qOywAXRrvpvtVkt6NiPdKDAZAGeMt+hJJK0oMBEA5rYvenNP9RknPHOfrrL0G9KnxvE31OknrIuKjY30xIpZLWi5JtqODsQHoyHg23ZeKzXbghNSq6M0yyVdLeq7scACU0HZJpgOSZhceC4BCODIOSICiAwlQdCABig4kQNGBBCg6kABFBxKg6EACFB1IwBHdv//E9seSJvKe9TmSPul4OP2QRR55tfLOiYi5R3+ySNEnyvaaiLh4qmWRR95k57HpDiRA0YEE+q3oy6doFnnkTWpeX+2jAyij32Z0AAVQdCABig4kQNGBBCg6kMB/AbBRqCcBZIexAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.matshow(out[0].reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "76ee9b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd80949040>"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.matshow(X_train[0].reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c3a6cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96897bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e402fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits['data']\n",
    "y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "36654446",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fbb48e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "70379b3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd804ef6d0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.matshow(digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ad03c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0.]])\n",
    "y_hat = np.array([[0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bbf51464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7008bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat[y_hat == 0] = 0.0001\n",
    "y_hat[y_hat == 1] = 0.9999\n",
    "term1 = y * np.log(y_hat)\n",
    "#print(f'_binary_cross_entropy: term1 shape is {term1.shape}')\n",
    "term2 = (1 - y) * np.log(1 - y_hat)\n",
    "#print(f'_binary_cross_entropy: term2 shape is {term2.shape}')\n",
    "bce = -np.mean(term1 + term2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63dfcb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f72dbccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69314718]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "800c192c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "516caa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = y_hat - y\n",
    "den = y_hat * (1 - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dedbc7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c74f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
